{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from __future__ import absolute_import, division, print_function\n\nimport tensorflow as tf\ntf.enable_eager_execution()\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport time\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"lyrics = pd.read_csv('../input/eminem.csv')\nlyrics.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04ad4a60d8e894fe524bcb0f581d709e7f491e79"},"cell_type":"code","source":"lyrics_text = lyrics['text']\nlyrics_text.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"057884a2dbfb29421c6bdced99c37889a0dde8ac"},"cell_type":"code","source":"text = '\\n\\n'.join(lyrics_text)\nprint ('Length of text: {} characters'.format(len(text)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ccd9e1687c4ad73bd23649eee4a96ee9cf21295e"},"cell_type":"code","source":"# Take a look at the first 250 characters in text\nprint(text[:250])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b9eb28b9bd44189236aa761e5b7982cc391257b"},"cell_type":"code","source":"# The unique characters in the file\nvocab = sorted(set(text))\nprint ('{} unique characters'.format(len(vocab)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b689a6102aa823caf4cec0e3bad9e1ef017dd97"},"cell_type":"code","source":"# Creating a mapping from unique characters to indices\nchar2idx = {u:i for i, u in enumerate(vocab)}\nidx2char = np.array(vocab)\n\ntext_as_int = np.array([char2idx[c] for c in text])\n\nprint('{')\nfor char,_ in zip(char2idx, range(20)):\n    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\nprint('  ...\\n}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07778c7711270a8e87854004f64ea557c7c6f36f"},"cell_type":"code","source":"# Show how the first 13 characters from the text are mapped to integers\nprint ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b58c7ddc5aed0e39f24fc53380eaef3065e646e5"},"cell_type":"code","source":"# The maximum length sentence we want for a single input in characters\nseq_length = 100\nexamples_per_epoch = len(text)//seq_length\n\n# Create training examples / targets\nchar_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n\nfor i in char_dataset.take(5):\n    print(idx2char[i.numpy()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b622508a9c1017868cc131a60672eac8ff1d085c"},"cell_type":"code","source":"sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n\nfor item in sequences.take(5):\n    print(repr(''.join(idx2char[item.numpy()])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7643c1cf9956cb03d941b475c0251797e884cf63"},"cell_type":"code","source":"def split_input_target(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\n\ndataset = sequences.map(split_input_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb62dbc179f1ce4b95f1c42ae4a65deb55e13cb6"},"cell_type":"code","source":"for input_example, target_example in  dataset.take(1):\n    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7a03ad2b48cd6b6bbf4afb512a8cc47f573b9dc"},"cell_type":"code","source":"for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n    print(\"Step {:4d}\".format(i))\n    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bdc8eeaea81a1acc209b30ecc4dbf0b172ef5b58"},"cell_type":"code","source":"# Batch size \nBATCH_SIZE = 64\nsteps_per_epoch = examples_per_epoch//BATCH_SIZE\n\n# Buffer size to shuffle the dataset\n# (TF data is designed to work with possibly infinite sequences, \n# so it doesn't attempt to shuffle the entire sequence in memory. Instead, \n# it maintains a buffer in which it shuffles elements).\nBUFFER_SIZE = 10000\n\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n\ndataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8fd8cd7134f19b6accbae74120bbca597e8d6dc7"},"cell_type":"code","source":"# Length of the vocabulary in chars\nvocab_size = len(vocab)\n\n# The embedding dimension \nembedding_dim = 256\n\n# Number of RNN units\nrnn_units = 1024","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"181e91caa3fadbc441b2338b9c673f234c57c8ff"},"cell_type":"code","source":"if tf.test.is_gpu_available():\n    rnn = tf.keras.layers.CuDNNGRU\nelse:\n    import functools\n    rnn = functools.partial(\n    tf.keras.layers.GRU, recurrent_activation='sigmoid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"228b884c6c4463f5ba0fb176f2fb9cf2946f93d6"},"cell_type":"code","source":"def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n    model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, \n                              batch_input_shape=[batch_size, None]),\n    rnn(rnn_units,\n        return_sequences=True, \n        recurrent_initializer='glorot_uniform',\n        stateful=True),\n    tf.keras.layers.Dense(vocab_size)\n  ])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa7ae4854422919b6f0987fb4418f18b9b995fe0"},"cell_type":"code","source":"model = build_model(\n  vocab_size = len(vocab), \n  embedding_dim=embedding_dim, \n  rnn_units=rnn_units, \n  batch_size=BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"550c534b66ddf0e5e39d17ec3443c17565bead24"},"cell_type":"code","source":"for input_example_batch, target_example_batch in dataset.take(1): \n    example_batch_predictions = model(input_example_batch)\n    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b87eda55ec85c4c010896f10054578f520e9671"},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c610b6a3f8df2887dca3a84e5c5e869eb5fccb44"},"cell_type":"code","source":"sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\nsampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c972ca7628746954a2ecdc5a01fd197ce44dc7e2"},"cell_type":"code","source":"sampled_indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b36a1c6999ed1195168d21d44116b190f2333f1c"},"cell_type":"code","source":"print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\nprint()\nprint(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7da9e4bea4ea3cd37c668c630fa02fe3fb0b066"},"cell_type":"code","source":"def loss(labels, logits):\n    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n\nexample_batch_loss  = loss(target_example_batch, example_batch_predictions)\nprint(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\") \nprint(\"scalar_loss:      \", example_batch_loss.numpy().mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a03bcd450473a496b9b406fba4b480353fcb0a48"},"cell_type":"code","source":"model.compile(\n    optimizer = tf.train.AdamOptimizer(),\n    loss = loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d0dd2150b76525c27ff6e18b834d267955265e1"},"cell_type":"code","source":"# Directory where the checkpoints will be saved\ncheckpoint_dir = './training_checkpoints'\n# Name of the checkpoint files\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63b54e5f426d13fd85add6b24ed326f85d19596c"},"cell_type":"code","source":"EPOCHS=3\nhistory = model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9fc7c823a1e512a8039126359d1e97ca446b746"},"cell_type":"code","source":"tf.train.latest_checkpoint(checkpoint_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d65f01bd55001bb942530ba347f480c15f31620"},"cell_type":"code","source":"model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n\nmodel.build(tf.TensorShape([1, None]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d33ed74eb6616dd2b8da1e3e8ed02ee89e85f774"},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3ebb6ee22bcc5e156bdaf2971b5fdd6591db491"},"cell_type":"code","source":"def generate_text(model, start_string):\n    # Evaluation step (generating text using the learned model)\n    \n    # Number of characters to generate\n    num_generate = 1000\n    \n    # Converting our start string to numbers (vectorizing) \n    input_eval = [char2idx[s] for s in start_string]\n    input_eval = tf.expand_dims(input_eval, 0)\n    \n    # Empty string to store our results\n    text_generated = []\n    \n    # Low temperatures results in more predictable text.\n    # Higher temperatures results in more surprising text.\n    # Experiment to find the best setting.\n    temperature = 0.3\n    \n    # Here batch size == 1\n    model.reset_states()\n    for i in range(num_generate):\n        predictions = model(input_eval)\n        # remove the batch dimension\n        predictions = tf.squeeze(predictions, 0)\n\n        # using a multinomial distribution to predict the word returned by the model\n        predictions = predictions / temperature\n        predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n\n        # We pass the predicted word as the next input to the model\n        # along with the previous hidden state\n        input_eval = tf.expand_dims([predicted_id], 0)\n\n        text_generated.append(idx2char[predicted_id])\n\n    return (start_string + ''.join(text_generated))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43521bea9f998e48630b8138ab3bfa3cd156dd58"},"cell_type":"code","source":"print(generate_text(model, start_string=u\"I'm a \"))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}